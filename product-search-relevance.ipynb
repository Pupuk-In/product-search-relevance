{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4b79b60",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3b5fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install datasets\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef346451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel, BertTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bec5f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"LazarusNLP/stsb_mt_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ec23f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dataset['train'] = dataset.pop('validation')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    dataset['validation'] = dataset.pop('test')\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0bac7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc4a12b",
   "metadata": {},
   "source": [
    "## Normalize the correlation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba09892",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cor = [cor['correlation'] for cor in dataset['train']]\n",
    "val_cor = [cor['correlation'] for cor in dataset['validation']]\n",
    "\n",
    "norm_train_cor = [float(i)/5.0 for i in train_cor]\n",
    "norm_val_cor = [float(i)/5.0 for i in val_cor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cb36b8",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4467c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "sentence_1 = [item['text_1'] for item in dataset['validation']]\n",
    "sentence_2 = [item['text_2'] for item in dataset['validation']]\n",
    "text_cat = [[str(x), str(y)] for x,y in zip(sentence_1, sentence_2)][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be22bd0",
   "metadata": {},
   "source": [
    "## Define STSBert Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc356b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STSBertModel(tf.keras.layers.Layer):\n",
    "    def __init__(self, model_name_or_path, **kwargs):\n",
    "        super(STSBertModel, self).__init__()\n",
    "        # loads transformers model\n",
    "        self.model = TFAutoModel.from_pretrained(model_name_or_path, **kwargs)\n",
    "\n",
    "    def call(self, inputs, normalize=True):\n",
    "        # runs model on inputs\n",
    "        model_output = self.model(inputs)\n",
    "        # Perform pooling. In this case, mean pooling.\n",
    "        embeddings = self.mean_pooling(model_output, inputs[\"attention_mask\"])\n",
    "        # normalizes the embeddings if wanted\n",
    "        if normalize:\n",
    "            embeddings = self.normalize(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] # First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = tf.cast(\n",
    "            tf.broadcast_to(tf.expand_dims(attention_mask, -1), tf.shape(token_embeddings)),\n",
    "            tf.float32\n",
    "        )\n",
    "        return tf.math.reduce_sum(token_embeddings * input_mask_expanded, axis=1) / tf.clip_by_value(tf.math.reduce_sum(input_mask_expanded, axis=1), 1e-9, tf.float32.max)\n",
    "\n",
    "    def normalize(self, embeddings):\n",
    "        embeddings, _ = tf.linalg.normalize(embeddings, 2, axis=1)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c52f20",
   "metadata": {},
   "source": [
    "## Embedding Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec3042e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Hugging Face model id\n",
    "model_id = 'indobenchmark/indobert-base-p2'\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_id, model_max_length=128)\n",
    "model = STSBertModel(model_id)\n",
    "\n",
    "# Run inference & create embeddings\n",
    "sentences = [\"Pupuk NPK\",\n",
    "           \"Pupuk Nitrogen\"]\n",
    "input_data = tokenizer(payload, padding=True, truncation=True, return_tensors='tf')\n",
    "sentence_embedding = model(input_data)\n",
    "\n",
    "print(sentence_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c429c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSequence(tf.keras.utils.Sequence):\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        similarity = [item['score'] for item in dataset]\n",
    "        self.label = [float(item)/5.0 for item in similarity]\n",
    "        self.sentence_1 = [item['text_1'] for item in dataset]\n",
    "        self.sentence_2 = [item['text_2'] for item in dataset]\n",
    "        self.text_cat = [[str(x), str(y)] for x, y in zip(self.sentence_1, self.sentence_2)]\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_cat)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        return self.label[idx]\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        inputs = self.tokenizer(\n",
    "            self.text_cat[idx],\n",
    "            padding='max_length',\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            return_tensors=\"tf\"\n",
    "        )\n",
    "        return inputs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "        return batch_texts, batch_y\n",
    "\n",
    "def collate_fn(texts):\n",
    "    num_texts = len(texts['input_ids'])\n",
    "    features = list()\n",
    "    for i in range(num_texts):\n",
    "        features.append({'input_ids':texts['input_ids'][i], 'attention_mask':texts['attention_mask'][i]})\n",
    "  \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8515b0e5",
   "metadata": {},
   "source": [
    "## Define CosineLoss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89285fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    return 1.0 - cosine_similarity(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89a8b6f",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eeb523",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import cosine_similarity\n",
    "\n",
    "# Huggingface model_id\n",
    "model_id = 'indobenchmark/indobert-base-p2'\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(model_id, model_max_length=128)\n",
    "bert_model = STSBertModel(model_id)\n",
    "\n",
    "\n",
    "# Tokenize input sentences\n",
    "train_encodings = tokenizer(dataset['train']['text_1'], dataset['train']['text_2'], truncation=True, padding=True)\n",
    "val_encodings = tokenizer(dataset['validation']['text_1'], dataset['validation']['text_2'], truncation=True, padding=True)\n",
    "train_labels = norm_train_cor\n",
    "val_labels = norm_val_cor\n",
    "\n",
    "# Create Tensorflow Datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    "))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    "))\n",
    "\n",
    "# Define the model inputs\n",
    "input_ids = Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = Input(shape=(None,), dtype=tf.int32, name=\"attention_mask\")\n",
    "token_type_ids = Input(shape=(None,), dtype=tf.int32, name=\"token_type_ids\")\n",
    "\n",
    "# Get the BERT model outputs\n",
    "bert_outputs = bert_model({\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"token_type_ids\": token_type_ids})\n",
    "\n",
    "# Add a dense layer for similarity classification\n",
    "dense = Dense(1, activation='sigmoid')(bert_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[input_ids, attention_mask, token_type_ids], outputs=dense)\n",
    "\n",
    "EPOCHS = 8\n",
    "LEARNING_RATE = 1e-6\n",
    "BATCH_SIZE = 8\n",
    "              \n",
    "# Compile the model with cosine similarity loss\n",
    "model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss=cosine_loss)\n",
    "\n",
    "# Tokenize and batch the data\n",
    "train_dataset = train_dataset.shuffle(len(dataset['train']['text_1'])).batch(BATCH_SIZE).repeat(4)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, epochs=EPOCHS, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601d1155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
