{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec18be0",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca21788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a61f23fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "base_model_name = \"cahya/bert-base-indonesian-522M\"\n",
    "dataset_name = \"LazarusNLP/stsb_mt_id\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e440ab25",
   "metadata": {},
   "source": [
    "## Load base model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b52ebde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at cahya/bert-base-indonesian-522M were not used when initializing TFBertModel: ['mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at cahya/bert-base-indonesian-522M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(base_model_name)\n",
    "model = TFBertModel.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0322e5",
   "metadata": {},
   "source": [
    "## Tokenizer test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "940ff20d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_test = ['Pupuk NPK','Pupuk Nitrogen']\n",
    "text_preprocessed = tokenizer(text_test, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"tf\")\n",
    "text_preprocessed.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12557f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 128), dtype=int32, numpy=\n",
       "array([[    3, 11994, 24540,  1028,     1,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2],\n",
       "       [    3, 11994, 10819,     1,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2]])>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessed['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "389429a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 128), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessed['token_type_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ed7e51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 128), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessed['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a8225d",
   "metadata": {},
   "source": [
    "## Base model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e243b5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results = model(text_preprocessed)\n",
    "test_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c34978b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.95307875]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_encoded = test_results['pooler_output']\n",
    "cosine_similarity([test_encoded[0]], [test_encoded[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b160747",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b1ae8019",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/Teguh/.cache/huggingface/datasets/LazarusNLP___parquet/LazarusNLP--stsb_mt_id-53495c8bc04ac9ed/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 608.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    validation: Dataset({\n",
       "        features: ['domain', 'data', 'type', 'score', 'correlation', 'text_1', 'text_2'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['domain', 'data', 'type', 'score', 'correlation', 'text_1', 'text_2'],\n",
       "        num_rows: 1379\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_name, name=\"en\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "188b4b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STSBertModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(STSBertModel, self).__init__()\n",
    "\n",
    "        self.bert_model = model\n",
    "        self.pooling_layer = tf.keras.layers.GlobalAveragePooling1D()\n",
    "\n",
    "    def call(self, input_data, training=False):\n",
    "        input_ids = input_data['input_ids']\n",
    "        attention_mask = input_data['attention_mask']\n",
    "\n",
    "        outputs = self.bert_model(input_ids, attention_mask=attention_mask, training=training)\n",
    "        pooled_output = self.pooling_layer(outputs.last_hidden_state)\n",
    "\n",
    "        return {'sentence_embedding': pooled_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8ced2968",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSequence(tf.data.Dataset):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "\n",
    "        similarity = [i['similarity_score'] for i in dataset]\n",
    "        self.label = [i/5.0 for i in similarity]\n",
    "        self.sentence_1 = [i['sentence1'] for i in dataset]\n",
    "        self.sentence_2 = [i['sentence2'] for i in dataset]\n",
    "        self.text_cat = [[str(x), str(y)] for x, y in zip(self.sentence_1, self.sentence_2)]\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.text_cat)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "\n",
    "        return tf.constant(self.label[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "\n",
    "        inputs = tokenizer(self.text_cat[idx], padding='max_length', max_length=128, truncation=True, return_tensors=\"tf\")\n",
    "        return inputs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y\n",
    "\n",
    "def collate_fn(texts):\n",
    "\n",
    "    num_texts = len(texts[0]['input_ids'])\n",
    "    features = list()\n",
    "    for i in range(num_texts):\n",
    "        features.append({'input_ids': texts[0]['input_ids'][i], 'attention_mask': texts[0]['attention_mask'][i]})\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a82fd3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSimilarityLoss(tf.keras.losses.Loss):\n",
    "\n",
    "    def __init__(self, loss_fct=tf.keras.losses.MeanSquaredError(), cos_score_transformation=tf.identity):\n",
    "        super(CosineSimilarityLoss, self).__init__()\n",
    "        self.loss_fct = loss_fct\n",
    "        self.cos_score_transformation = cos_score_transformation\n",
    "        self.cos = tf.keras.losses.CosineSimilarity(axis=1)\n",
    "\n",
    "    def call(self, input, label):\n",
    "        embedding_1 = tf.stack([inp[0] for inp in input])\n",
    "        embedding_2 = tf.stack([inp[1] for inp in input])\n",
    "\n",
    "        output = self.cos_score_transformation(self.cos(embedding_1, embedding_2))\n",
    "\n",
    "        return self.loss_fct(output, tf.squeeze(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7fb02457",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class DataSequence with abstract methods _inputs, element_spec",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[47], line 8\u001b[0m, in \u001b[0;36mmodel_train\u001b[1;34m(dataset, epochs, learning_rate, bs)\u001b[0m\n\u001b[0;32m      5\u001b[0m criterion \u001b[38;5;241m=\u001b[39m CosineSimilarityLoss()\n\u001b[0;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m----> 8\u001b[0m train_sequence \u001b[38;5;241m=\u001b[39m \u001b[43mDataSequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_generator(train_sequence\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m, output_signature\u001b[38;5;241m=\u001b[39m(tf\u001b[38;5;241m.\u001b[39mTensorSpec(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m128\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32), tf\u001b[38;5;241m.\u001b[39mTensorSpec(shape\u001b[38;5;241m=\u001b[39m(), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)))\n\u001b[0;32m     10\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mbatch(bs)\u001b[38;5;241m.\u001b[39mmap(collate_fn)\u001b[38;5;241m.\u001b[39mprefetch(tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n",
      "\u001b[1;31mTypeError\u001b[0m: Can't instantiate abstract class DataSequence with abstract methods _inputs, element_spec"
     ]
    }
   ],
   "source": [
    "def model_train(dataset, epochs, learning_rate, bs):\n",
    "\n",
    "    model = STSBertModel()\n",
    "\n",
    "    criterion = CosineSimilarityLoss()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    train_sequence = DataSequence(dataset)\n",
    "    train_dataset = tf.data.Dataset.from_generator(train_sequence.__getitem__, output_signature=(tf.TensorSpec(shape=(None, 128), dtype=tf.int32), tf.TensorSpec(shape=(), dtype=tf.float32)))\n",
    "    train_dataset = train_dataset.batch(bs).map(collate_fn).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    best_acc = 0.0\n",
    "    best_loss = 1000\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0.0\n",
    "\n",
    "        for train_data, train_label in tqdm(train_dataset):\n",
    "            with tf.GradientTape() as tape:\n",
    "                output = model(train_data, training=True)['sentence_embedding']\n",
    "                loss = criterion(output, train_label)\n",
    "\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "            total_loss_train += loss.numpy()\n",
    "\n",
    "        print(f'Epoch {epoch + 1} | Loss: {total_loss_train / len(dataset): .3f}')\n",
    "        model.trainable = False\n",
    "\n",
    "    return model\n",
    "\n",
    "EPOCHS = 8\n",
    "LEARNING_RATE = 1e-6\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Train the model\n",
    "trained_model = model_train(dataset, EPOCHS, LEARNING_RATE, BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
